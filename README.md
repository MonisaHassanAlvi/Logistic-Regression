# Logistic-Regression
A Python project to demonstrate logistic regression with and without regularization.

# Tools & Applications
- Pycharm Community Edition 2020.3.3
- Python 3.7/3.8

# Description

 ## Logistic Regression
 ![image](https://user-images.githubusercontent.com/85407775/121799611-866d0200-cc46-11eb-95a8-1ee5eba41e31.png)
 
- ## Visualizing the Data
Before starting to implement any algorithm, it is always good to visualize the data if possible. Write a code to display a figure like Figure 1, where the axes are the two exam scores, and the positive and negative examples are shown with different markers.

![image](https://user-images.githubusercontent.com/85407775/121799663-c9c77080-cc46-11eb-8907-5c1e31698f60.png)

- ## Implementation
- ### sigmoid function
![image](https://user-images.githubusercontent.com/85407775/121799725-18750a80-cc47-11eb-814e-456032edcb39.png)

Implement the sigmoid function. Your code should also work with Matrices and vectors. For a matrix, your function should perform the sigmoid function on every element.

- ### Cost function and gradient
![image](https://user-images.githubusercontent.com/85407775/121799771-64c04a80-cc47-11eb-838e-fac614369f69.png)

- ### Learning Parameters and plotting
Implement the gradient descent and find optimal parameters. On optimal parameters you should see the cost is about 0.203. Use final theta value to plot the decision boundary on the training data, resulting in a figure similar to Figure 2.

![image](https://user-images.githubusercontent.com/85407775/121799858-eca65480-cc47-11eb-89f2-2b884fd58b61.png)

- ### Evaluating logistic regression
![image](https://user-images.githubusercontent.com/85407775/121799875-270ff180-cc48-11eb-9785-a3c802baabc3.png)

## Regularized logistic regression
![image](https://user-images.githubusercontent.com/85407775/121799899-50308200-cc48-11eb-863e-19112741155d.png)

- ## Visualizing the data
![image](https://user-images.githubusercontent.com/85407775/121799965-a56c9380-cc48-11eb-861c-57c12339e7e8.png)
![image](https://user-images.githubusercontent.com/85407775/121799983-c1703500-cc48-11eb-8280-871f274754aa.png)

- ## Feature Mapping
![image](https://user-images.githubusercontent.com/85407775/121800000-d5b43200-cc48-11eb-9048-6bc2b55ebeae.png)

- ## Cost function and gradient
![image](https://user-images.githubusercontent.com/85407775/121800023-f1b7d380-cc48-11eb-93d2-f246cbfd1945.png)

- ### Learning Parameters and Plotting
Implement the gradient descent and find optimal parameters. Use final theta value to plot the decision boundary on the training data, resulting in a figure similar to Figure 4.

![image](https://user-images.githubusercontent.com/85407775/121800071-275cbc80-cc49-11eb-8a82-f633740ebee7.png)

- ### Try Different Regularization Parameters
![image](https://user-images.githubusercontent.com/85407775/121800122-7145a280-cc49-11eb-87e6-4e0dee3aa8ef.png)
![image](https://user-images.githubusercontent.com/85407775/121800128-7f93be80-cc49-11eb-9cb0-7b3e8b8c8359.png)
![image](https://user-images.githubusercontent.com/85407775/121800137-8c181700-cc49-11eb-83c0-8c84fe6a4494.png)
